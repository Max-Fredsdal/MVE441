{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fdddd16b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import KFold, cross_val_score,GridSearchCV\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from KNN import KNN\n",
    "from KNN import LDA\n",
    "from KNN import RandomForest\n",
    "import matplotlib.pyplot as plt\n",
    "from plotter import BoxPlot, OptimismPlot\n",
    "\n",
    "from helper import Evaluation, doubleCV, CVErrorVSHyperparam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25d8dc6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KNN done tuning\n",
      "LDA Done tuning\n",
      "randomForest Done tuning\n",
      "small k (tuned) final test error: 0.0125\n",
      "large k (tuned) final test error: 0.1725\n",
      "LDA (tuned) final test error: 0.0925\n",
      "Random Forest (tuned) final test error: 0.0050\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/sklearn/base.py:486: UserWarning: X has feature names, but KNeighborsClassifier was fitted without feature names\n",
      "  warnings.warn(\n",
      "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/sklearn/base.py:486: UserWarning: X has feature names, but KNeighborsClassifier was fitted without feature names\n",
      "  warnings.warn(\n",
      "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/sklearn/base.py:486: UserWarning: X has feature names, but LinearDiscriminantAnalysis was fitted without feature names\n",
      "  warnings.warn(\n",
      "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/sklearn/base.py:486: UserWarning: X has feature names, but RandomForestClassifier was fitted without feature names\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    # Load the data\n",
    "    df = pd.read_csv('data/Numbers.txt', delimiter=' ')\n",
    "    X = df.iloc[:, 1:]\n",
    "    y = df.iloc[:, 0]\n",
    "    \n",
    "    # train test split\n",
    "    X_t, X_tst, y_t, y_tst = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "    \n",
    "\n",
    "    outerCV = KFold(n_splits=10, shuffle=True, random_state=42)\n",
    "    foldDataTraining = []\n",
    "    foldDataTest = []\n",
    "    #Plot data\n",
    "    allTestErrors = {}\n",
    "    optimism = {}\n",
    "    bestModels = {}\n",
    "    finalTestErrors = {}\n",
    "\n",
    "    #Get the folded training and test data\n",
    "    for train_idx, test_idx in outerCV.split(X_t, y_t):\n",
    "        X_train = X.iloc[train_idx].to_numpy()\n",
    "        y_train = y.iloc[train_idx].to_numpy().reshape(-1, 1)\n",
    "\n",
    "        X_test = X.iloc[test_idx].to_numpy()\n",
    "        y_test = y.iloc[test_idx].to_numpy().reshape(-1, 1)\n",
    "\n",
    "        train_combined = np.hstack((y_train, X_train))\n",
    "        test_combined = np.hstack((y_test, X_test))\n",
    "\n",
    "        foldDataTraining.append(train_combined)\n",
    "        foldDataTest.append(test_combined)\n",
    "    \n",
    "    \"\"\"Evaluation without tuning\"\"\"\n",
    "\n",
    "    #KNN \n",
    "    smallKClassifier = KNN(n_neighbors=5)\n",
    "    largeKClassifier = KNN(n_neighbors=20)\n",
    "    smallKtrainingErrorsNoTuning,smallKtestErrorsNoTuning,_,_ = Evaluation(smallKClassifier,foldDataTraining,foldDataTest)\n",
    "    largeKtrainingErrorsNoTuning,largeKtestErrorsNoTuning,_,_ = Evaluation(largeKClassifier,foldDataTraining,foldDataTest)\n",
    "    allTestErrors[\"Small k (no tuning)\"] = smallKtestErrorsNoTuning\n",
    "    allTestErrors[\"Large k (no tuning)\"] = largeKtestErrorsNoTuning\n",
    "\n",
    "    KNNOptimismSmallK = np.array(smallKtestErrorsNoTuning) - np.array(smallKtrainingErrorsNoTuning)\n",
    "    KNNOptimismLargeK = np.array(largeKtestErrorsNoTuning) - np.array(largeKtrainingErrorsNoTuning)\n",
    "\n",
    "    optimism[\"Small k (no tuning)\"] = KNNOptimismSmallK\n",
    "    optimism[\"Large k (no tuning)\"] = KNNOptimismLargeK\n",
    "\n",
    "    #LDA \n",
    "    ldaClassifier = LDA() \n",
    "    ldaTrainingErrors, ldaTestErrors, _, _ = Evaluation(ldaClassifier, foldDataTraining, foldDataTest)\n",
    "    allTestErrors[\"LDA (no tuning)\"] = ldaTestErrors\n",
    "\n",
    "    LDAOptimism = np.array(ldaTestErrors) - np.array(ldaTrainingErrors)\n",
    "\n",
    "    optimism[\"LDA (no tuning)\"] = LDAOptimism\n",
    "\n",
    "    #RandomForest\n",
    "    rfClassifier = RandomForest()\n",
    "    rfTrainingErrors, rfTestErrors, _, _ = Evaluation(rfClassifier, foldDataTraining, foldDataTest)\n",
    "    allTestErrors[\"Random Forest (no tuning)\"] = rfTestErrors\n",
    "\n",
    "    rfOptimism = np.array(rfTestErrors) - np.array(rfTrainingErrors)\n",
    "\n",
    "    optimism[\"Random Forest (no tuning)\"] = rfOptimism\n",
    "\n",
    "    \"\"\"Evaluation with tuning\"\"\"\n",
    "\n",
    "    #KNN\n",
    "    \n",
    "    smallKparamGrid = {'n_neighbors': list(range(1,11))} #k = 1,2,...10 --> Flexible\n",
    "    largeKparamGrid = {'n_neighbors': list(range(50, 151, 10))} #k = 50,60..150 --> Rigid\n",
    "\n",
    "    _, smallKtrainingErrors, smallKtestErrors, smallBestModel = doubleCV(foldDataTraining, foldDataTest, KNeighborsClassifier(), smallKparamGrid)\n",
    "    _, largeKtrainingErrors, largeKtestErrors, largeBestModel = doubleCV(foldDataTraining, foldDataTest, KNeighborsClassifier(), largeKparamGrid)\n",
    "\n",
    "    \n",
    "\n",
    "    KNNOptimismSmallK_Tuned = np.array(smallKtestErrors) - np.array(smallKtrainingErrors)\n",
    "    KNNOptimismLargeK_Tuned = np.array(largeKtestErrors) - np.array(largeKtrainingErrors)\n",
    "\n",
    "    allTestErrors[\"Small k (tuned)\"] = smallKtestErrors\n",
    "    allTestErrors[\"Large k (tuned)\"] = largeKtestErrors\n",
    "    optimism[\"Small k (tuned)\"] = KNNOptimismSmallK_Tuned\n",
    "    optimism[\"Large k (tuned)\"] = KNNOptimismLargeK_Tuned\n",
    "    bestModels[\"small k (tuned)\"] = smallBestModel\n",
    "    bestModels[\"large k (tuned)\"] = largeBestModel\n",
    "    \n",
    "    \n",
    "\n",
    "    print(\"KNN done tuning\")\n",
    "\n",
    "    #LDA\n",
    "    ldaParamGrid = [\n",
    "    {'solver': ['svd']}, \n",
    "    {'solver': ['lsqr', 'eigen'], 'shrinkage': [None, 'auto']}\n",
    "    ]\n",
    "    _, ldaTrainingErrors, ldaTestErrors, ldaBestModel = doubleCV(foldDataTraining, foldDataTest, LinearDiscriminantAnalysis(), ldaParamGrid)\n",
    "    allTestErrors[\"LDA (tuned)\"] = ldaTestErrors\n",
    "\n",
    "    LDAOptimism_Tuned = np.array(ldaTestErrors) - np.array(ldaTrainingErrors)\n",
    "    optimism[\"LDA (tuned)\"] = LDAOptimism_Tuned\n",
    "    bestModels[\"LDA (tuned)\"] = ldaBestModel\n",
    "\n",
    "    print(\"LDA Done tuning\")\n",
    "\n",
    "    #Random Forest\n",
    "    rfParamGrid = {\n",
    "    'n_estimators': [20,50, 100],          # number of trees\n",
    "    'max_depth': [None, 20,50],         # control overfitting\n",
    "    'max_features': ['sqrt'],  # feature selection per split\n",
    "    'min_samples_split': [2, 5,10]          # min samples for splitting\n",
    "    }\n",
    "    _, rfTrainingErrors, rfTestErrors, rfBestModel = doubleCV(foldDataTraining, foldDataTest, RandomForestClassifier(), rfParamGrid)\n",
    "    allTestErrors[\"Random Forest (tuned)\"] = rfTestErrors\n",
    "\n",
    "    rfOptimism_Tuned = np.array(rfTestErrors) - np.array(rfTrainingErrors)\n",
    "    optimism[\"Random Forest (tuned)\"] = rfOptimism_Tuned\n",
    "    bestModels[\"Random Forest (tuned)\"] = rfBestModel\n",
    "\n",
    "    print(\"randomForest Done tuning\")\n",
    "\n",
    "    \n",
    "    \"\"\"Evaluate range of hyperparameter values to get plot\"\"\"\n",
    "    #rangeOfk = np.arange(1,10)\n",
    "    #testErrorsForDifferentK = CVErrorVSHyperparam(KNN, 'n_neighbors', rangeOfk, foldDataTraining, foldDataTest)\n",
    "\n",
    "    #plt.plot(rangeOfk, testErrorsForDifferentK, marker='o')\n",
    "    #plt.xlabel(\"k\")\n",
    "    #plt.ylabel(\"CV Error\")\n",
    "    #plt.title(\"Unbiased CV Error vs k\")\n",
    "    #plt.grid(True)\n",
    "    #plt.show()\n",
    "\n",
    "    # \"Get plots\"\n",
    "    # BoxPlot(allTestErrors)\n",
    "    # OptimismPlot(optimism)\n",
    "    \n",
    "    # calculate the final test error for each model\n",
    "    for name, model in bestModels.items():\n",
    "        y_pred = model.predict(X_tst)\n",
    "        finalTestError = np.mean(np.where(y_pred != y_tst, 1, 0))\n",
    "        finalTestErrors[name] = finalTestError\n",
    "        print(f\"{name} final test error: {finalTestError}\")\n",
    "    \n",
    "\n",
    "main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
